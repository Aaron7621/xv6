## 实验二

实验的意义：在进程进入内核态的时候，使用进程自己的内核页表，而不是所有进程共享一个内核页表。

1. kvmmap panic的原因：可能是因为内存泄漏。在free进程内存资源的时候，一定要仔细检查是不是所有东西都free掉了。
2. 内核栈的分配时机：这个是灵活的。可以在进程模块初始化的时候分配（kalloc）好所有内核栈，也可以在一个进程需要被拉起的时候再分配。后者则需要在回收进程，freeproc()的时候把内核栈回收掉。
3. 内核栈的映射：可以只映射某一个进程的内核栈到进程内核页表中；也可以把所有内核栈都映射到内核全局页表中，也可以两者都映射。关键的是内核栈只能有一个，映射能有多个，同时也要搞清楚什么时候换地址空间，换了之后当前页表是否有内核栈的映射以供使用。
4. free内核页表的方式：如果内核栈的映射在全局内核页表中，那就没必要在拉起和结束每个进程的时候分配和回收内核栈，只需要管理好进程内核页表中的映射即可。如果是每次拉起进程才分配内核栈，并且进程内核栈的映射只存在于进程内核页表中，就需要在freeroc的时候释放内核栈并删除映射
5. kvmpa是根据全局内核页表翻译一个va。别人的代码中有对它进行修改，改为多接受一个pagetable_t的方式，这样会导致其他模块调用这个函数的时候都需要多传入一个pagetable_t参数。之所以这么设计是因为该答案的实现是在拉起进程的时候分配内核栈，并且内核栈映射只存在于进程内核页表中。所以回收时需要接受当前内核进程页表为参数，找到内核栈的物理地址并回收。
     而如果把内核栈的映射同时放在全局内核页表和进程内核页表，就没有修改kvmpa的必要。甚至没有回收内核栈的必要。



## 实验六：COW

1. 在Fork的时候不为子进程分配实际物理页，而是把子进程页表的PTE全部指向父进程的物理页（子进程直接完全复制父进程的页表）。同时把子进程和父进程的PTE读权限取消，并标记一个COW位。这一步在vm.c/uvmcopy()中完成

2. 当page fault时，检查page fault是否是合法的COW page。如果是，则根据当前触发page fault的va，获取响应的实物理页。page fault的处理在trap.c/usertrap中完成。

3. 根据COW va获取实复制页的操作在kalloc.c/get_pa_decre_ref()中完成。首先kalloc中需要维护一个各page引用计数的数组，表明每个page正在被多少个进程使用。

   1. *显然，当kalloc()调用，页初分配时，设引用计数为1*
   2. *当kfree调用，页回收时，引用计数 -= 1.如果减1后引用计数为0，则执行真正的页回收（把页放回空闲页链表中）。如果减一后不为0，说明该页还有其他进程再使用，kfree不需要再做其他处理。*

   get_pa_decre_ref()函数先检查当前COW page的引用计数。
           如果为1，表明当前页只有当前进程使用，则直接把当前的COW页改成普通页（消除COW标记，开启写权限）。
           如果页引用>1，表明还有其他进程在使用该COW page。所以为当前进程重新分配一个物理页，复制原COW页的内容，并把页表中指向该页的PTE的COW位取消，同时赋写权限。最后原COW页引用计数 -= 1.

4. 虽然此时还没到锁的一章。但是本实验对引用计数数组正确加锁至关重要。否则在多线程操作该数组时，不正确的加锁会导致读取的页引用有误，导致某些页永远不会被释放，造成内存泄漏。

## 实验七：MultiThread

### UThread

实验目的是实验一个用户级的线程切换程序

- 这是一个用户态下的线程切换程序，或者可以理解成是一个协作式的线程调度——各线程主动放弃对CPU的占用。

- 这是一个用户进程，虽然它也会被中断，可能运行在不同的cpu上。但xv6中用户进程在用户态的线程都是串行进行的。虽然现在一个用户进程有多个用户线程，但他们不会并发。**所以不需要考虑锁的问题**。

  *但是换句话来说，现在实现的多线程，除了还不能在多个核上并发运行这一点，其余的角度看已经实现了一个用户进程的多线程——线程切换代价很小（不需要trap到内核中），线程之间共用进程的地址空间*

- 实现

  - 初始化各线程时把线程的函数地址写到context.ra，栈地址（数组的末尾地址）写道context.sp，其余callee saved寄存器（context中的）清0
  - scheduler的时候调u_swtch
  - u_swtch中完成context的切换（换ra，换栈，换callee saved寄存器）

### Using Threads

- 哈希表的构成：并不是真正意义上的哈希表，只是一个存放key-value pair的特殊的数据结构而已

- 一共分为5个桶。每个桶都存放了一个entry结构体指针的链表的头节点。所以整个哈希表就是一个大小为5的数组，数组每个元素都是一个*entry链表的头节点

- 往哈希表里加key的时候，先找到key所在的桶（链表），然后用头插法插到该桶对应的链表中。
  头插法的步骤：定义一个链表节点e，e -> next = head，head = e。

- 多线程put时却key的原因：当两个线程put的节点属于同一个桶，可能两个节点e1，e2的next同时指向链表头节点，然后链表头节点最终会是e1、e2中的一个，假如是e1，则e2则永远没办法从新链表的头节点往后遍历的过程中获得，即丢失了。（但并没有内存泄漏），只是链表形成了这样的形态：

  ![多线程+头插法导致链表错误插入](D:\GitHubLocalRepository\xv6\note\picture\多线程+头插法导致链表错误插入.png)

- 可以给整个table加锁，也可以给每个桶加锁。识别出一些一定不会产生并发错误的代码，让锁的粒度尽可能细以提高性能

### Barrier 屏障

这个实验就是为下一节的sleep-wakeup机制做铺垫。实现上，相当于是调用了Linux中pthread的sleep和wakeup完成屏障的实现。最核心的问题：

*为什么pthread_cond_wait原语需要传一个锁的参数？*

下一节的sleep-wakeup有解答

## 实验八：Lock

本实验一言概之：就是让细化锁的粒度，提高并发度。但是不同场景下的锁优化面临的难度是截然不同的

### kalloc优化

- 原本场景是多核CPU共享、维护同一个空闲物理页链表。这样分配物理页的场景是单线程、串行的。当分配、回收操作密集时，会产生很多锁冲突。
- 细化对空闲物理页表锁：把单一空闲物理也链表分成per-cpu的链表。每个cpu维护只需要自己的空闲链表，每个链表有一把锁。把对单一链表的大锁拆分了。
- 当一个cpu链表为空时，就要遍历其他CPU链表找到空闲的页表节点，“偷”一个到自己的链表中，并分配出去。显然遍历其他cpu的链表时需要获得对应链表的锁。

### buffer cache优化

- 原本的场景是：文件系统和内存之间的有一个cache层，bcache管理和维护了一个buf对象的数组。每个buf对象主要是维护了一个cache（就是一个数组），与cache相关的信息，如对应的blockid，引用计数，睡眠锁（一个buf cache一次只能被一个线程使用）等。
  对于管理所有buf对象的bcache，有一把大锁。当外部请求buf对象或者释放buf对象时，都要获取bcache的锁进行分配和回收操作。
- 实验的目的就是拆分这把bcache的大锁，用hash桶的策略，根据不同buf的blockid映射到不同的哈希桶中，从而对每个bucket加锁，提高并行度。
- 本实验和kalloc实验最大的不同时，kalloc的锁优化是简单地将管理空闲page的链表拆分，每个cpu管理一个链表。当每个cpu需要处理回收和分配页的时候，只要自己的链表还有元素，直接返回就可以，是完全不需要管别的cpu的链表的。换言之每个page都是一样的，cpu分自己的page时，只要自己手中不是没page了，就可以直接分，完全不需要管别的cpu的page。
  而buffer cache的共享是**真正的共享**，整个bcache对象都是一个共享对象，**没办法拆分成per-cpu的对象**。原因是每个buf对象都不同，都对应着一个blockid。每个cpu中的线程都有可能获得任何一个blockid对应的cache。
- 整体框架是：把bcache维护的一条双向链表，转化成维护一个hash bucket，即一个数组，一共有NBUCKET个元素，每个元素是一个bucket（用链表来存）的头节点（可以是伪头节点）。
  - 在初始化时，把所有的buf都放到第一个bucket中
  - 需要get buf的时候，先找blockid对应的bucket中有没有cache好的。没有的话就遍历**所有buf**，找到LRU且引用计数为0的buf，把它驱逐。如果该buf的blockid所在桶和当前blockid对应的桶不是一个，还要把该buf节点从原链表删除，添加到当前对应桶的链表中。
  - 抛弃了双向链表的存储，转为存储每个buf的最后使用时间。通过这个方式来找LRU buf。原本总是会把LRU的放在末尾，最近使用的放在表头。使用了时间计数的方法后，release buf的操作就不需要操作链表了，只需要更新buf的最近使用时间，并减一下引用计数即可。

- 这个实验最tricky的地方在于如何正确地在buf get的流程中给每个桶加锁。锁保持时间过短可能会产生致命的错误。加大锁的窗口期又很可能退化会原来的大锁的效率，或者可能造成死锁。
- 实验的测试程序中中也给了测试线程很多限制，这充分说明本实验的锁细粒度化很难做到非常严格的无并发错误。实际上做完本实验下来最大的感受是：还不如前面的一把大锁。。。因为本身buf cache也只有30个。分配和回收的过程，理论上冲突并不会很大。但是可以减去很多并发错误的可能性

## 实验九：文件系统

### 扩充单个文件地址空间

本来inode中的data block只有直接块和一层索引。现在在此基础上再牺牲一个直接块的地址，添加一个二级索引。模仿Linux中混合索引。

实现上，将让出直接块的一个entry用来放二级索引的第一级索引的blockno，在第一级索引block中，一共可以放256个blockno，这256个blockno分别指向第二级索引的blockno。第二级索引的256个blockno则是真正的data block的blockno。

因此单个文件地址空间扩大了256*256 - 1个block。

### 实现软链接（符号链接）

回忆xv6本身链接（硬链接）的实现，概括就是：将原文件的inum，添加到目标文件夹的目录项中，并起一个名字。所以显然硬链接中，链接的对象是一定存在的（因为目标目录中都存了该文件对应的inum了）

软链接的核心是，把要链接的路径（即原路径）的字符串，单独利用作为一个块保存起来。可以理解成把原路径的字符串当成当成一个文件，然后把这个文件的inode对应的inum添加到目标目录的目录项下。

对比硬链接，硬链接是将原文件自身的inum保存的目标目录中，软链接的目标目录存的只是一个路径字符串（组成的文件）。

所以软连接的声明不一定要满足链接到的文件一定存在，因为声明时期只是定义一个文件，文件里包含路径字符串而已。当然文件类型会单独标记为软链接。

在打开（open）文件的时候，将用户传入的字符串利用namei找到对应的文件的inode。如果是软链接类型，则需要从该文件中再读出链接到的路径，然后继续namei找该路径对应的文件。

如果找到的文件仍然是软链接，则需要一直找下去。可以设置一个阈值，超出阈值后则认为存在环，停止查找返回失败。

当然，如果不能从软链接文件中的路径找到有效文件，也是返回失败

实验的测试中，还专门增加了一种omode：NOFOLLOW。意思是仅打开软链接文件本身，而不用去找（follow）软链接保存的路径中的文件。其实这本身也是给我们的一种提示：即**软链接就是一个文件，这个文件仅保存链接到的文件路径的字符串**。

## 虚拟内存

