## 实验二

实验的意义：在进程进入内核态的时候，使用进程自己的内核页表，而不是所有进程共享一个内核页表。

1. kvmmap panic的原因：可能是因为内存泄漏。在free进程内存资源的时候，一定要仔细检查是不是所有东西都free掉了。
2. 内核栈的分配时机：这个是灵活的。可以在进程模块初始化的时候分配（kalloc）好所有内核栈，也可以在一个进程需要被拉起的时候再分配。后者则需要在回收进程，freeproc()的时候把内核栈回收掉。
3. 内核栈的映射：可以只映射某一个进程的内核栈到进程内核页表中；也可以把所有内核栈都映射到内核全局页表中，也可以两者都映射。关键的是内核栈只能有一个，映射能有多个，同时也要搞清楚什么时候换地址空间，换了之后当前页表是否有内核栈的映射以供使用。
4. free内核页表的方式：如果内核栈的映射在全局内核页表中，那就没必要在拉起和结束每个进程的时候分配和回收内核栈，只需要管理好进程内核页表中的映射即可。如果是每次拉起进程才分配内核栈，并且进程内核栈的映射只存在于进程内核页表中，就需要在freeroc的时候释放内核栈并删除映射
5. kvmpa是根据全局内核页表翻译一个va。别人的代码中有对它进行修改，改为多接受一个pagetable_t的方式，这样会导致其他模块调用这个函数的时候都需要多传入一个pagetable_t参数。之所以这么设计是因为该答案的实现是在拉起进程的时候分配内核栈，并且内核栈映射只存在于进程内核页表中。所以回收时需要接受当前内核进程页表为参数，找到内核栈的物理地址并回收。
     而如果把内核栈的映射同时放在全局内核页表和进程内核页表，就没有修改kvmpa的必要。甚至没有回收内核栈的必要。



## 实验六：COW

1. 在Fork的时候不为子进程分配实际物理页，而是把子进程页表的PTE全部指向父进程的物理页（子进程直接完全复制父进程的页表）。同时把子进程和父进程的PTE读权限取消，并标记一个COW位。这一步在vm.c/uvmcopy()中完成

2. 当page fault时，检查page fault是否是合法的COW page。如果是，则根据当前触发page fault的va，获取响应的实物理页。page fault的处理在trap.c/usertrap中完成。

3. 根据COW va获取实复制页的操作在kalloc.c/get_pa_decre_ref()中完成。首先kalloc中需要维护一个各page引用计数的数组，表明每个page正在被多少个进程使用。

   1. *显然，当kalloc()调用，页初分配时，设引用计数为1*
   2. *当kfree调用，页回收时，引用计数 -= 1.如果减1后引用计数为0，则执行真正的页回收（把页放回空闲页链表中）。如果减一后不为0，说明该页还有其他进程再使用，kfree不需要再做其他处理。*

   get_pa_decre_ref()函数先检查当前COW page的引用计数。
           如果为1，表明当前页只有当前进程使用，则直接把当前的COW页改成普通页（消除COW标记，开启写权限）。
           如果页引用>1，表明还有其他进程在使用该COW page。所以为当前进程重新分配一个物理页，复制原COW页的内容，并把页表中指向该页的PTE的COW位取消，同时赋写权限。最后原COW页引用计数 -= 1.

4. 虽然此时还没到锁的一章。但是本实验对引用计数数组正确加锁至关重要。否则在多线程操作该数组时，不正确的加锁会导致读取的页引用有误，导致某些页永远不会被释放，造成内存泄漏。

## 实验七：MultiThread

### UThread

实验目的是实验一个用户级的线程切换程序

- 这是一个用户态下的线程切换程序，或者可以理解成是一个协作式的线程调度——各线程主动放弃对CPU的占用。

- 这是一个用户进程，虽然它也会被中断，可能运行在不同的cpu上。但xv6中用户进程在用户态的线程都是串行进行的。虽然现在一个用户进程有多个用户线程，但他们不会并发。**所以不需要考虑锁的问题**。

  *但是换句话来说，现在实现的多线程，除了还不能在多个核上并发运行这一点，其余的角度看已经实现了一个用户进程的多线程——线程切换代价很小（不需要trap到内核中），线程之间共用进程的地址空间*

- 实现

  - 初始化各线程时把线程的函数地址写到context.ra，栈地址（数组的末尾地址）写道context.sp，其余callee saved寄存器（context中的）清0
  - scheduler的时候调u_swtch
  - u_swtch中完成context的切换（换ra，换栈，换callee saved寄存器）

### Using Threads

- 哈希表的构成：并不是真正意义上的哈希表，只是一个存放key-value pair的特殊的数据结构而已

- 一共分为5个桶。每个桶都存放了一个entry结构体指针的链表的头节点。所以整个哈希表就是一个大小为5的数组，数组每个元素都是一个*entry链表的头节点

- 往哈希表里加key的时候，先找到key所在的桶（链表），然后用头插法插到该桶对应的链表中。
  头插法的步骤：定义一个链表节点e，e -> next = head，head = e。

- 多线程put时却key的原因：当两个线程put的节点属于同一个桶，可能两个节点e1，e2的next同时指向链表头节点，然后链表头节点最终会是e1、e2中的一个，假如是e1，则e2则永远没办法从新链表的头节点往后遍历的过程中获得，即丢失了。（但并没有内存泄漏），只是链表形成了这样的形态：

  ![多线程+头插法导致链表错误插入](D:\GitHubLocalRepository\xv6\note\picture\多线程+头插法导致链表错误插入.png)

- 可以给整个table加锁，也可以给每个桶加锁。识别出一些一定不会产生并发错误的代码，让锁的粒度尽可能细以提高性能

### Barrier 屏障

这个实验就是为下一节的sleep-wakeup机制做铺垫。实现上，相当于是调用了Linux中pthread的sleep和wakeup完成屏障的实现。最核心的问题：

*为什么pthread_cond_wait原语需要传一个锁的参数？*

下一节的sleep-wakeup有解答

## 虚拟内存

